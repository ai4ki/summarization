{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be03dbe7",
   "metadata": {},
   "source": [
    "# Summarization\n",
    "\n",
    "*Updated: 01/31/2022*\n",
    "\n",
    "This notebook allows you to summarize texts by using two different approaches.\n",
    "\n",
    "The first approach is called 'Extractive Summarization' (ES). As its name suggests, ES employs algorithms, which try to identify and extract the most relevant sentences from a text. ES is a computationally  efficient and fast summarization approach. \n",
    "\n",
    "The second approach is called 'Abstractive Summarization' (AS). This approach is based on (large) language models (LLM) for which the so called transformer architecture has become the de-facto standard. Having 'seen' enough pairs of text and a summary during training and leveraging its general language capabilities, an LLM can generate a summary that can contain original sentences verbatim alongside paraphrased or newly generated text. In contrast to ES, AS is very compute expensive.      \n",
    "\n",
    "Scientific texts strongly differ from most other texts in structure, form, and style. As LLMs are usually trained on vast troves of diverese texts scraped off the internet, we can't expect them to deal particularly well with scientific literature (unless we fine-tune them for this task). \n",
    "\n",
    "It is, therefore, strongly recommended to first use ES algorithms as they return original sentences from the text. AS might, instead, be a good starting point in case you have to write a summary, say, for a project proposal.     \n",
    "\n",
    "## Working with Jupyter notebooks\n",
    "\n",
    "In case you are not familiar with Jupyter notebooks, this is how to go about it: In order to execute a piece of code, click inside a cell (the ones with `[]` to the left) and press Shift+Enter. Wait until the cell is done--that's when the `*` in `[]` turned into a number--and move on to the next cell.\n",
    "\n",
    "If you get inconceivable error messages or the notebook gets stuck, choose \"Restart & Clear Output\" in the \"Kernel\" dropdown-menu above and start afresh. \n",
    "\n",
    "___\n",
    "**Please help us to improve this tool by [emailing us](mailto:ai4ki.dev@gmail.com?subject=ai4ki-tools:%20Summarization) your update ideas or error reports.**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0998c2",
   "metadata": {},
   "source": [
    "## Preparation: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70223922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from summarization_utils import *\n",
    "from transformers import pipeline\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9eddd0",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cea0e3",
   "metadata": {},
   "source": [
    "### Step 1: Load your text\n",
    "You can provide your text in the `.pdf` or `.txt`-formats.\n",
    "\n",
    "Note that we use Adobe's Document Services API for extracting PDF content. This API is not a free service and thus requires credentials. You can regsiter for a free trial  [here](https://www.adobe.io/apis/documentcloud/dcsdk/). In case you need support with authentication of your credentials, [email us](mailto:ai4ki.dev@gmail.com?subject=Summarization:%20Authentication%20Issues). \n",
    "\n",
    "In future updates of this notebook, we will try to implement an open source alternative like [this one](https://github.com/allenai/s2orc-doc2json).\n",
    "\n",
    "*Run the following cell, and enter the name of your file.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db58d5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter the name of your file\n",
    "infile = input('Enter filename: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f920f055",
   "metadata": {},
   "source": [
    "#### 1.1: Fetch an article from the Web (optional)\n",
    "*Run the following cell, enter the full URL of the article you want to summarize, and proceed directly to summarization.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202d8c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = input('Enter full URL: ')\n",
    "text_to_summarize = fetch_article(url)\n",
    "text_to_summarize = text_to_summarize.strip().replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a819527d",
   "metadata": {},
   "source": [
    "### Step 2: Convert your text into a machine readable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8378bf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preprocessing data...\")\n",
    "fulltext, _, _, headers = preprocess_data(infile, base_path='./')\n",
    "if headers:\n",
    "    print(f\"Found the following section headers in {infile}\")\n",
    "    for i, h in enumerate(headers):\n",
    "        print(f\"{i}: {h['header']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2229c5",
   "metadata": {},
   "source": [
    "### Step 3: Choose text section for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7e46f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chap_slct = input(\"Enter section number or -1 for full text: \") \n",
    "if chap_slct == '-1':\n",
    "    text_to_summarize = fulltext.strip()\n",
    "else:\n",
    "    chap_data = headers[int(chap_slct)]\n",
    "    text_to_summarize = fulltext[chap_data['idx_start']:chap_data['idx_end']].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429be54d",
   "metadata": {},
   "source": [
    "## Approach I: Extractive Summariztaion\n",
    "\n",
    "In this approach you can choose between the following summarization algorithms:  [LRS](https://pypi.org/project/lexrank/), [LSA](https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python), [LUHN](https://pypi.org/project/sumy/), and [KLS](https://pypi.org/project/sumy/). Follow the links, if you want to learn more about how these algorithms work. We found the LSA algorithm to produce the best results: It consistently extracted sentences, which captured core ideas of the text.  \n",
    "\n",
    "*Run the following cell and enter the desired length of your summary as well as the summarization algorithm.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8882ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose lenght of summary (i.e. number of sentences)\n",
    "len_sum = input('Number of sentences: ')\n",
    "\n",
    "# Chose extraction algorithm\n",
    "ext_alg = input('Extractive algorithm: ')\n",
    "\n",
    "# Create the summary \n",
    "summary = extractive_summarizers(text_to_summarize, algorithm=ext_alg, max_len=int(len_sum), lang='german')\n",
    "\n",
    "# Print the summary\n",
    "print(f\"==> SUMMARY:\")\n",
    "for sentence in summary:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98daf802",
   "metadata": {},
   "source": [
    "## Approach II: Abstractive Summarization\n",
    "\n",
    "You have two options here: first, summarization with a pre-trained T5 transformer model; second, summarization with GPT-2. T5 creates summaries, which often contain originial or only slightly paraphrased sentences. GPT-2, in contrast, mostly generates new text and, therefore, is more of an abstractive summarizer. We use the [Huggingface transformer library](https://huggingface.co/docs/transformers/index) for both options. \n",
    "\n",
    "Large language models set a limit to the length their combined input and output can have. This length is measured in 'tokens'. A token does not always correspond to a word, but, as a rule of thumb, you can think of 100 tokens corresponding to 75 words. Typical limits are 512, 1024, or 2048 tokens (see [here](https://beta.openai.com/docs/introduction/key-concepts) for a short introduction into tokenization).\n",
    "\n",
    "In order to be able to summarize longer texts, we first split the input text into equal chunks of appropriate size; we then summarize the first chunk, add this summary to the second chunk, and repeat this procedure until the last chunk. We set the maximum chunk size to 400 tokens (~300 words). This ensures that we stay safely below the models' limits (512 tokens and 1024 tokens for T5 and GPT-2, respectively). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af71584",
   "metadata": {},
   "source": [
    "### Option 1: Summarization with T5\n",
    "\n",
    "*Run the following cell to get a T5 summary of your selected text. Be patient, as completion might take up to a minute.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74e6ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into chunks, which fit into the transformer context window\n",
    "chunks = chunk_text(text_to_summarize, max_tokens=400)\n",
    "\n",
    "# Initialize the HuggingFace summarization pipeline\n",
    "summarizer = pipeline('summarization', model='t5-base', tokenizer='t5-base', framework='tf')\n",
    "\n",
    "# Generate the summary\n",
    "summary = ''\n",
    "for chunk in chunks:\n",
    "    prompt = summary + ' ' + chunk\n",
    "    sum_tmp = summarizer(prompt.strip().replace('\\n',' '), min_length=30, max_length=100, do_sample=False)\n",
    "    summary = sum_tmp[0]['summary_text']\n",
    "print('==> SUMMARY:')\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f473ab",
   "metadata": {},
   "source": [
    "### Option 2: Summarization with GPT-2\n",
    "\n",
    "The output of GPT-2 can be tuned with different parameters, the most important of which is `temperature`. Roughly speaking, temperature controls the randomness of the output. In our case, values close to zero will more likely create extractive summaries, while larger values allow for more abstractive summaries. You can experiment with different values by changing the parameter in the cell below.\n",
    "\n",
    "Note that GPT-2 can generate text, which is essentially bunk (at least with respect to the summarization task at hand). Therefore, you might have to go through a couple of trials until you get something useful. Also note that you can try summarization with GPT-3 in this [notebook](https://github.com/ai4ki/transformer-playground.git).  \n",
    "\n",
    "*Run the following cell to have GPT-2 summarize your text. Be patient, as completion might take up to a minute.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14413edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into chunks, which fit into the transformer context window\n",
    "chunks = chunk_text(text_to_summarize, max_tokens=400)\n",
    "\n",
    "# Generate the summary\n",
    "summary = ''\n",
    "for chunk in chunks:\n",
    "    prompt = summary + ' ' + chunk\n",
    "    sum_tmp = gpt2_summarizer(prompt.strip().replace('\\n',' '), max_sum_length=100, temperature=0.7)\n",
    "    summary = sum_tmp\n",
    "print('==> SUMMARY:')\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0518e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ai4ki]",
   "language": "python",
   "name": "conda-env-ai4ki-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
