{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be03dbe7",
   "metadata": {},
   "source": [
    "# Summarization\n",
    "\n",
    "*Updated: 01/27/2022*\n",
    "\n",
    "This notebook allows you to summarize texts by using two different approaches.\n",
    "\n",
    "The first approach is called 'Extractive Summarization' (ES). As its name suggests, ES employs algorithms, which try to identify and extract the most relevant sentences from a text. ES is a computationally  efficient and fast summarization approach. \n",
    "\n",
    "The second approach is called 'Abstractive Summarization' (AS). This approach is based on (large) language models (LLM) for which the so called transformer architecture has become the de-facto standard. Having 'seen' enough pairs of text and a summary during training and leveraging its general language capabilities, an LLM can generate a summary that can contain original sentences verbatim alongside rewritten or newly generated text. In contrast to ES, AS is very compute expensive.      \n",
    "\n",
    "Scientific texts strongly differ from most other texts in structure, form, and style. As LLMs are usually trained on vast troves of diverese texts scraped off the internet, we can't expect them to deal particularly well with scientific literature (unless we fine-tune them for this task). \n",
    "\n",
    "It is, therefore, strongly recommended to first use ES algorithms as they return original sentences from the text. AS might, instead, be a good starting point in case you have to write a summary, say, for a project proposal.     \n",
    "\n",
    "## Working with Jupyter notebooks\n",
    "\n",
    "In case you are not familiar with Jupyter notebooks, this is how to go about it: In order to execute a piece of code, click inside a cell (the ones with `[]` to the left) and press Shift+Enter. Wait until the cell is done--that's when the `*` in `[]` turned into a number--and move on to the next cell.\n",
    "\n",
    "If you get inconceivable error messages or the notebook gets stuck, choose \"Restart & Clear Output\" in the \"Kernel\" dropdown-menu above and start afresh. \n",
    "\n",
    "___\n",
    "**Please help us to improve this tool by [emailing us](mailto:ai4ki.dev@gmail.com?subject=ai4ki-tools:%20Summarization) your update ideas or error reports.**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0998c2",
   "metadata": {},
   "source": [
    "## Preparation: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70223922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from summarization_utils import *\n",
    "from transformers import pipeline\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9eddd0",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cea0e3",
   "metadata": {},
   "source": [
    "### Step 1: Load your text\n",
    "You can provide your text in the `.pdf` or `.txt`-formats.\n",
    "\n",
    "Note that we use Adobe's Document Services API for extracting PDF content. This API is not a free service and thus requires credentials. You can regsiter for a free trial  [here](https://www.adobe.io/apis/documentcloud/dcsdk/). In case you need support with authentication of your credentials, [email us](mailto:ai4ki.dev@gmail.com?subject=Summarization:%20Authentication%20Issues). \n",
    "\n",
    "In future updates of this notebook, we will try to implement an open source alternative like [this one](https://github.com/allenai/s2orc-doc2json).\n",
    "\n",
    "*Run the following cell, and enter the name of your file.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db58d5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter the name of your file\n",
    "infile = input('Enter filename: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f920f055",
   "metadata": {},
   "source": [
    "#### 1.1: Fetch an article from the Web (optional)\n",
    "*Run the following cell, enter the full URL of the article you want to summarize, and proceed directly to summarization.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202d8c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = input('Enter full URL: ')\n",
    "text_to_summarize = fetch_article(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a819527d",
   "metadata": {},
   "source": [
    "### Step 2: Convert your text into a machine readable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8378bf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preprocessing data...\")\n",
    "fulltext, _, _, headers = preprocess_data(infile, base_path='./')\n",
    "if headers:\n",
    "    print(f\"Found the following section headers in {infile}\")\n",
    "    for i, h in enumerate(headers):\n",
    "        print(f\"{i}: {h['header']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2229c5",
   "metadata": {},
   "source": [
    "### Step 3: Choose text section for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7e46f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chap_slct = input(\"Enter section number or -1 for full text: \") \n",
    "if chap_slct == '-1':\n",
    "    text_to_summarize = fulltext\n",
    "else:\n",
    "    chap_data = headers[int(chap_slct)]\n",
    "    text_to_summarize = fulltext[chap_data['idx_start']:chap_data['idx_end']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429be54d",
   "metadata": {},
   "source": [
    "## Approach I: Extractive Summariztaion\n",
    "\n",
    "*Enter the desired length of your summary and the summarization algorithm. You can choose between [LRS](https://pypi.org/project/lexrank/), [LSA](https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python), [LUHN](https://pypi.org/project/sumy/), and [KLS](https://pypi.org/project/sumy/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8882ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose lenght of summary (i.e. number of sentences)\n",
    "len_sum = input('Number of sentences: ')\n",
    "\n",
    "# Chose extraction algorithm\n",
    "ext_alg = input('Extractive algorithm: ')\n",
    "\n",
    "# Create the summary \n",
    "summary = extractive_summarizers(text_to_summarize, method=ext_alg, max_len=int(len_sum), lang='german')\n",
    "\n",
    "# Print the summary\n",
    "print(f\"==> SUMMARY:\")\n",
    "for sentence in summary:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98daf802",
   "metadata": {},
   "source": [
    "## Approach II: Abstractive Summarization\n",
    "\n",
    "Large Language Models like transformers set a limit to the length their combined input and output can have. This length is measured in 'tokens'. A token does not always correspond to a word, but as a rule of thumb you can think of 100 tokens corresponding to 75 words. Typical limits are 512, 1024, or 2048 tokens (see [here](https://beta.openai.com/docs/introduction/key-concepts) for a short introduction).\n",
    "\n",
    "In the case of Huggingface's [Summarization Pipeline](https://huggingface.co/transformers/main_classes/pipelines.html#summarizationpipeline), which we use here, the limit is currently at 512 tokens. In order to to be able to summarize longer texts, we first split the input text into chunks of sizes smaller than 512 tokens; we then  summarize each chunk separately; finally, we concatenate the individual chunk summaries to get the full summary.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74e6ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into chunks, which fit into the transformer context window\n",
    "chunks = chunk_text(text_to_summarize)\n",
    "    \n",
    "# Choose model (facebook/bart-large-cnn, t5-small, t5-base, t5-large, t5-3b, t5-11b)\n",
    "model = 't5-base'\n",
    "\n",
    "# Initialize the HuggingFace summarization pipeline\n",
    "summarizer = pipeline('summarization', model=model, tokenizer=model, framework='tf')\n",
    "\n",
    "# Create summary for each chunk and concatenate\n",
    "summary = ''\n",
    "for chunk in chunks:\n",
    "    sum_tmp = summarizer(chunk, min_length=30, max_length=60)\n",
    "    summary += sum_tmp[0]['summary_text']\n",
    "# Print summarized text\n",
    "print('==> SUMMARY:')\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c731be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ai4ki]",
   "language": "python",
   "name": "conda-env-ai4ki-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
